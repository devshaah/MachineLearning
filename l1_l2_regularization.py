# -*- coding: utf-8 -*-
"""L1_L2_Regularization.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tYskbvQTaeV_5i9Fr9q-dLER-aUqozhR
"""

#HELPS OVERCOME PROBLEM OF OVERFITTING(model doesnâ€™t generalize well from our training data to unseen data)
# WE ADD A TERM IN LINEAR EQUATION TO MINMAX THE ERROR
# IN L1 ->  lambda summation(|theta|)
# IN L2 ->  lambda summation(theta^2)

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

dataset = pd.read_csv('Melbourne_housing_FULL.csv')
dataset.head()

cols_to_use = ['Suburb', 'Rooms', 'Type', 'Method', 'SellerG', 'Regionname', 'Propertycount',
               'Distance', 'CouncilArea', 'Bedroom2', 'Bathroom', 'Car', 'Landsize', 'BuildingArea', 'Price']
dataset = dataset[cols_to_use]
dataset.isna().sum()

cols_to_fill_zero = ['Propertycount', 'Distance', 'Bedroom2', 'Bathroom', 'Car']
dataset[cols_to_fill_zero] = dataset[cols_to_fill_zero].fillna(0)
dataset['Landsize'] = dataset['Landsize'].fillna(dataset.Landsize.mean())
dataset['BuildingArea'] = dataset['BuildingArea'].fillna(dataset.BuildingArea.mean())
dataset.dropna(inplace=True)
dataset = pd.get_dummies(dataset, drop_first=True)

X = dataset.drop('Price', axis=1)
y = dataset['Price']
from sklearn.model_selection import train_test_split
train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.3, random_state=2)

from sklearn.linear_model import LinearRegression
reg = LinearRegression().fit(train_X, train_y)
reg.score(test_X, test_y)

#the test score is very low but train score is good, which means LinearRegression is clearly overfitting data
#for this we have Lasso (l1 regularization)
# there is also Ridge (l2 regularization)
from sklearn.linear_model import Lasso
model = Lasso(alpha=50, max_iter=100, tol=0.1)
model.fit(train_X, train_y)
model.score(test_X, test_y)